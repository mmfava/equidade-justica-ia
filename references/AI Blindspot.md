---
url: https://aiblindspot.media.mit.edu/images/AI_Cards_2021.pdf
title: AI Blindspot
author: MIT
alias: AI-blindspots-mit
created: 2025-01-22
updated: 2025-01-22T19:12
group: outros
---

## LICENÇAS CRIATIVAS, EQUIPE E AGRADECIMENTOS

Os cartões **AI Blindspot** foram desenvolvidos por Ania Calderon, Dan Taber, Hong Qu e Jeff Wen durante o programa **Assembly 2019** do **Berkman Klein Center** e **MIT Media Lab**.

Este projeto não teria sido possível sem as estimulantes conversas com a **Assembly Cohort** e os **Assembly Advisors**. Agradecemos especialmente a **Shira Chung** pelo design dos cartões AI Blindspot, **Lara Taber** pelo design do logotipo e **Mariel Calderon** pelo design do Processo de Descoberta AI Blindspot. Também somos gratos a **Friederike Schuur, Walter Frick e Erich Ludwig** por suas valiosas contribuições em sessões de feedback, bem como a **John Bowers** por suas edições impecáveis. Finalmente, queremos agradecer a **Hilary Ross** pelo seu apoio contínuo e orientação durante o programa.

Este trabalho está licenciado sob a licença **Creative Commons Attribution 4.0 International**.

[www.aiblindspot.com](https://chatgpt.com/c/www.aiblindspot.com)

---

## INTRODUÇÃO AO AI BLINDSPOT

Os **Blindspots** (pontos cegos) da IA são falhas no fluxo de trabalho de uma equipe que podem gerar consequências prejudiciais e não intencionais. Eles podem surgir devido a nossos **preconceitos inconscientes** ou a **desigualdades estruturais** embutidas na sociedade.

Esses pontos cegos podem ocorrer antes, durante ou depois do desenvolvimento de um modelo, desde sua concepção até sua implementação. As consequências são difíceis de prever, mas tendem a afetar negativamente **comunidades historicamente marginalizadas**.

Como qualquer **ponto cego**, os blindspots de IA são universais—ninguém está imune a eles—mas os danos podem ser mitigados se **agirmos intencionalmente** para evitá-los.

---

## O QUE QUEREMOS DIZER POR IA?

A inteligência artificial se tornou uma categoria abrangente para sistemas automatizados de tomada de decisão que extraem **padrões, insights e previsões** de grandes conjuntos de dados. Embora aspirem a imitar e automatizar o julgamento humano, a maioria dos algoritmos chamados de IA são, na verdade, modelos imperfeitos **suscetíveis a erros e vieses**.

O risco de delegar decisões sociais e comerciais importantes à IA expõe as pessoas a **tratamento desigual**, pois esses algoritmos aparentemente imparciais são desenvolvidos por cientistas, engenheiros e empresas cujos **dados e práticas podem amplificar preconceitos históricos**.

A justiça requer **vigilância cuidadosa** em todos os setores, especialmente entre **pesquisadores, engenheiros, organizações que implantam IA e defensores dos direitos humanos**. Acima de tudo, devemos proteger e apoiar as pessoas cujas vidas são **impactadas pela IA**.

---

## COMO USAR OS CARTÕES?

É difícil prever **onde** podem surgir **erros e falhas** no desenvolvimento da IA ou como preveni-los.

Esses cartões incentivam discussões para **descobrir possíveis blindspots** durante o **planejamento, construção e implementação** de sistemas de IA. Cada cartão contém:

- Um **resumo** de um possível ponto cego;
- **Ações** que você pode tomar para corrigi-lo;
- **Exemplos** de impacto na vida real;
- **Quem** você deve consultar;
- Um **QR code** com recursos adicionais.

Não há uma **solução única** para evitar blindspots, então incluímos um **“coringa”** para inspirar novas reflexões.

---

## PROCESSO DE DESCOBERTA DE BLINDSPOTS EM IA

O **processo de descoberta** envolve **10 etapas** que ajudam organizações a identificar e mitigar **viés inconsciente** e **desigualdades estruturais** antes de implantar sistemas de IA. Ele abrange as fases:

### **1. Planejamento**

- **Propósito**: Qual o objetivo da IA?
- **Dados Representativos**: Os dados refletem todas as comunidades afetadas?
- **Abusabilidade**: Como seu sistema pode ser explorado de forma maliciosa?
- **Privacidade**: Como proteger os dados pessoais?

### **2. Construção**

- **Discriminação por Proxy**: Há variáveis que funcionam como proxies para grupos vulneráveis?
- **Explicabilidade**: Como os resultados do modelo podem ser interpretados?
- **Critérios de Otimização**: A métrica de sucesso pode causar impactos negativos?

### **3. Implantação**

- **Erro de Generalização**: O modelo continuará relevante ao longo do tempo?
- **Direito de Contestação**: As pessoas podem questionar decisões automatizadas?

### **4. Monitoramento**

- **Supervisão**: Quem será responsável por garantir o uso ético?
- **Consulta**: Como as comunidades afetadas são envolvidas no processo?

---

## EXEMPLOS DE BLINDSPOTS EM IA

### **Propósito**

❖ **Exemplo**: Durante o surto de **Ebola em 2014**, epidemiologistas usaram registros telefônicos para monitorar o deslocamento populacional. No entanto, como o vírus se espalha por **contato direto**, essa abordagem **não ajudou a conter a epidemia**.

**Como evitar?** ✔ Definir claramente o **problema e objetivo**; ✔ Avaliar se a IA é realmente **adequada**; ✔ Incluir pessoas afetadas para definir **critérios de sucesso**.

---

### **Dados Representativos**

❖ **Exemplo**: Um **algoritmo de detecção de câncer de pele** foi mais preciso que dermatologistas em peles **claras**, mas falhou em detectar cânceres em peles **escuras**, pois o **banco de dados não era diversificado**.

**Como evitar?** ✔ Garantir que os dados **não amplifiquem vieses históricos**; ✔ Incluir **vozes diversas** na coleta de dados.

---

### **Abusabilidade**

❖ **Exemplo**: **Reconhecimento facial** foi promovido como uma ferramenta de **segurança**, mas na **China** tem sido usado para **rastrear e controlar minorias étnicas**.

**Como evitar?** ✔ Criar cenários hipotéticos para prever **uso malicioso**; ✔ Realizar **testes adversários**.

---

### **Privacidade**

❖ **Exemplo**: Para receber **benefícios sociais**, pessoas de baixa renda fornecem **dados pessoais**, que podem ser explorados por empresas para **negar moradia ou empregos**.

**Como evitar?** ✔ Implementar **aprendizado federado** e **privacidade diferencial**; ✔ Exigir **consentimento explícito**.

---

### **Discriminação por Proxy**

❖ **Exemplo**: Um **algoritmo de saúde** usado nos EUA subestimou a gravidade de doenças em **pacientes negros** porque utilizava **custos médicos como proxy para necessidade de tratamento**, ignorando desigualdades no acesso à saúde.

**Como evitar?** ✔ Identificar e remover **variáveis correlacionadas com grupos vulneráveis**.

---

### **Explicabilidade**

❖ **Exemplo**: A **lei de crédito dos EUA** dá aos consumidores o direito de **entender** seus escores de crédito. No entanto, modelos complexos muitas vezes não fornecem **justificativas compreensíveis**.

**Como evitar?** ✔ Escolher modelos que sejam **mais interpretáveis**; ✔ Criar ferramentas de **explicação contrafactual**.

